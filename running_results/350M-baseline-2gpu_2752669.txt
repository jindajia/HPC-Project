no change     /N/slate/jindjia/Btools/miniforge3/condabin/conda
no change     /N/slate/jindjia/Btools/miniforge3/bin/conda
no change     /N/slate/jindjia/Btools/miniforge3/bin/conda-env
no change     /N/slate/jindjia/Btools/miniforge3/bin/activate
no change     /N/slate/jindjia/Btools/miniforge3/bin/deactivate
no change     /N/slate/jindjia/Btools/miniforge3/etc/profile.d/conda.sh
no change     /N/slate/jindjia/Btools/miniforge3/etc/fish/conf.d/conda.fish
no change     /N/slate/jindjia/Btools/miniforge3/shell/condabin/Conda.psm1
no change     /N/slate/jindjia/Btools/miniforge3/shell/condabin/conda-hook.ps1
no change     /N/slate/jindjia/Btools/miniforge3/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /N/slate/jindjia/Btools/miniforge3/etc/profile.d/conda.csh
modified      /N/u/jindjia/BigRed200/.bashrc

==> For changes to take effect, close and re-open your current shell. <==

Added mamba to /N/u/jindjia/BigRed200/.bashrc

==> For changes to take effect, close and re-open your current shell. <==

++ scontrol show hostnames
++ sort -u
+ HOSTNAMES=nid0703
+ HOSTLIST=
+ FIRST_HOST=
+ for HOST in $HOSTNAMES
+ '[' -z '' ']'
+ FIRST_HOST=nid0703
+ HOST_ARRAY+=($HOST)
+ HOSTLIST=nid0703,
+ HOSTLIST=nid0703
+ MASTER_ADDR=nid0703
+ srun --nodes=1 --gres=gpu:2 --export=ALL,GPUS_PER_NODE=2,MASTER_ADDR=nid0703,NNODES=1 bash -c ./run_dist.sh
+ MASTER_PORT=6000
+ WORLD_SIZE=2
+ NODE_RANK=0
+ VOCAB_FILE=/N/scratch/jindjia/thepile/vocab.json
+ MERGE_FILE=/N/scratch/jindjia/thepile/merges.txt
+ DATA_PATH=/N/scratch/jindjia/thepile/pile_text_document
+ CHECKPOINT_PATH=/N/scratch/jindjia/checkpoint/Course/hpc-course/Project
+ WANDB_DIR=/tmp/2752669/wandb
+ TENSORBOARD_DIR=/tmp/2752669/tensorboard
+ trap copy_files EXIT
+ DISTRIBUTED_ARGS='
    --nproc_per_node 2     --nnodes 1     --node_rank 0     --master_addr nid0703     --master_port 6000 '
+ MODEL_ARGS='
    --num-layers 24     --hidden-size 1024     --num-attention-heads 16     --seq-length 2048     --max-position-embeddings 2048 '
+ TRAINING_ARGS='
    --tensor-model-parallel-size 1     --pipeline-model-parallel-size 1     --micro-batch-size 8     --global-batch-size 16     --train-iters 80000 '
+ OPTIMIZER_ARGS='
    --lr 0.0003     --lr-decay-iters 70000     --lr-decay-style cosine     --min-lr 0.00003     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-08     --weight-decay .1     --lr-warmup-fraction 0.01     --clip-grad 1.0     --loss-scale 0     --loss-scale-window 1000     --hysteresis 2     --min-loss-scale 1     --fp16     --use-distributed-optimizer '
+ DATA_ARGS='
    --data-path /N/scratch/jindjia/thepile/pile_text_document     --vocab-file /N/scratch/jindjia/thepile/vocab.json     --merge-file /N/scratch/jindjia/thepile/merges.txt     --data-cache-path /tmp/2752669/data_cache     --distributed-storage '
+ OUTPUT_ARGS='
    --log-interval 10     --timing-log-level 2     --log-timers-to-tensorboard     --tensorboard-dir /tmp/2752669/tensorboard     --tensorboard-log-interval 1     --save-interval 5002     --eval-interval 100     --eval-iters 10     --log-timers-to-tensorboard     --log-validation-ppl-to-tensorboard     --log-throughput     --wandb-project DEBUG     --wandb-save-dir /tmp/2752669/wandb     --wandb-exp-name 350M-baseline '
+ QUANTIZE_ARGS='
    --no-async-tensor-model-parallel-allreduce     --recompute-activations     --recompute-granularity selective '
+ export OMP_NUM_THREADS=16
+ OMP_NUM_THREADS=16
+ cd /N/slate/jindjia/LLM/Megatron-LM-Final-Design
+ torchrun --nproc_per_node 2 --nnodes 1 --node_rank 0 --master_addr nid0703 --master_port 6000 pretrain_gpt.py --num-layers 24 --hidden-size 1024 --num-attention-heads 16 --seq-length 2048 --max-position-embeddings 2048 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --micro-batch-size 8 --global-batch-size 16 --train-iters 80000 --lr 0.0003 --lr-decay-iters 70000 --lr-decay-style cosine --min-lr 0.00003 --adam-beta1 0.9 --adam-beta2 0.95 --adam-eps 1e-08 --weight-decay .1 --lr-warmup-fraction 0.01 --clip-grad 1.0 --loss-scale 0 --loss-scale-window 1000 --hysteresis 2 --min-loss-scale 1 --fp16 --use-distributed-optimizer --data-path /N/scratch/jindjia/thepile/pile_text_document --vocab-file /N/scratch/jindjia/thepile/vocab.json --merge-file /N/scratch/jindjia/thepile/merges.txt --data-cache-path /tmp/2752669/data_cache --distributed-storage --log-interval 10 --timing-log-level 2 --log-timers-to-tensorboard --tensorboard-dir /tmp/2752669/tensorboard --tensorboard-log-interval 1 --save-interval 5002 --eval-interval 100 --eval-iters 10 --log-timers-to-tensorboard --log-validation-ppl-to-tensorboard --log-throughput --wandb-project DEBUG --wandb-save-dir /tmp/2752669/wandb --wandb-exp-name 350M-baseline --no-async-tensor-model-parallel-allreduce --recompute-activations --recompute-granularity selective --distributed-backend nccl --save /N/scratch/jindjia/checkpoint/Course/hpc-course/Project --load /N/scratch/jindjia/checkpoint/Course/hpc-course/Project --exit-duration-in-mins 2840
Zarr-based strategies will not be registered because of missing packages
using world size: 2, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. True
  add_position_embedding .......................... True
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  apply_rope_fusion ............................... True
  async_tensor_model_parallel_allreduce ........... False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  bias_swiglu_fusion .............................. True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  check_for_nan_in_loss_and_grad .................. True
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  clone_scatter_output_in_embedding ............... True
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  context_parallel_size ........................... 1
  data_cache_path ................................. /tmp/2752669/data_cache
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 2
  data_path ....................................... ['/N/scratch/jindjia/thepile/pile_text_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  delay_grad_reduce ............................... True
  delay_param_gather .............................. False
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_storage ............................. True
  distributed_timeout_minutes ..................... 10
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  encoder_num_layers .............................. 24
  encoder_seq_length .............................. 2048
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... False
  eval_interval ................................... 100
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 2840
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_model_parallel_size ...................... 1
  ffn_hidden_size ................................. 4096
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 16
  gq_group_size_inter ............................. 128
  gq_group_size_intra ............................. 512
  gradient_accumulation_fusion .................... True
  gradient_quantization_bits_inter ................ 4
  gradient_quantization_bits_intra ................ 8
  gradients_quantization_start_iteration .......... 0
  group_query_attention ........................... False
  hadamard_transform .............................. False
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1024
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  kv_channels ..................................... 64
  lazy_mpu_init ................................... None
  load ............................................ /N/scratch/jindjia/checkpoint/Course/hpc-course/Project
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 10
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_progress .................................... False
  log_throughput .................................. True
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... 0.0
  loss_scale_window ............................... 1000.0
  lr .............................................. 0.0003
  lr_decay_iters .................................. 70000
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. 0.01
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  manual_gc ....................................... False
  manual_gc_eval .................................. True
  manual_gc_interval .............................. 0
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 2048
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... /N/scratch/jindjia/thepile/merges.txt
  micro_batch_size ................................ 8
  min_loss_scale .................................. 1.0
  min_lr .......................................... 3e-05
  moe_grouped_gemm ................................ False
  nccl_communicator_config_path ................... None
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... LayerNorm
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... None
  num_layers ...................................... 24
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 1
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  overlap_param_gather ............................ False
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 1
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... learned_absolute
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  quantized_gradients ............................. False
  quantized_weights ............................... False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... selective
  recompute_method ................................ None
  recompute_num_layers ............................ None
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_attention_gate ............................ 1
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_return_doc_ids ............................ False
  retro_verify_neighbor_count ..................... True
  retro_workdir ................................... None
  rotary_percent .................................. 1.0
  rotary_seq_len_interpolation_factor ............. None
  sample_rate ..................................... 1.0
  save ............................................ /N/scratch/jindjia/checkpoint/Course/hpc-course/Project
  save_interval ................................... 5002
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 2048
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  spec ............................................ None
  split ........................................... 969, 30, 1
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.1
  swiglu .......................................... False
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. /tmp/2752669/tensorboard
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  timing_log_level ................................ 2
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. None
  tokenizer_type .................................. GPT2BPETokenizer
  tp_comm_bulk_dgrad .............................. True
  tp_comm_bulk_wgrad .............................. True
  tp_comm_overlap ................................. False
  tp_comm_overlap_cfg ............................. None
  tp_comm_split_ag ................................ True
  tp_comm_split_rs ................................ True
  train_data_path ................................. None
  train_iters ..................................... 80000
  train_samples ................................... None
  transformer_impl ................................ local
  transformer_pipeline_model_parallel_size ........ 1
  untie_embeddings_and_output_weights ............. False
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_distributed_optimizer ....................... True
  use_flash_attn .................................. False
  use_mcore_models ................................ False
  use_one_sent_docs ............................... False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /N/scratch/jindjia/thepile/vocab.json
  vocab_size ...................................... None
  wandb_exp_name .................................. 350M-baseline
  wandb_project ................................... DEBUG
  wandb_save_dir .................................. /tmp/2752669/wandb
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  weight_quantization_bits ........................ 4
  world_size ...................................... 2
  wq_group_size ................................... 2048
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 1
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
2024-04-23 03:08:58.898092: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-04-23 03:08:58.898135: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-04-23 03:08:58.899141: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-04-23 03:08:58.927668: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-23 03:09:03.637359: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
> setting tensorboard ...
wandb: Currently logged in as: jiajinda001 (jinda-personal). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.2
wandb: Run data is saved locally in /tmp/2752669/wandb/wandb/run-20240423_030916-ztd451ty
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 350M-baseline
wandb: ⭐️ View project at https://wandb.ai/jinda-personal/DEBUG
wandb: 🚀 View run at https://wandb.ai/jinda-personal/DEBUG/runs/ztd451ty
> initialized tensor model parallel with size 1
> initialized pipeline model parallel with size 1
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory '/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/core/datasets'
make: Nothing to be done for 'default'.
make: Leaving directory '/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 0.123 seconds
> compiling and loading fused kernels ...
>>> done with compiling and loading fused kernels. Compilation time: 1.326 seconds
/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
time to initialize megatron (seconds): 32.179
[after megatron is initialized] datetime: 2024-04-23 03:09:27 
building GPT model ...
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 355919872
INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (355919872 elements):
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.embedding.position_embeddings.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.embedding.word_embeddings.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.final_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.final_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.self_attention.dense.weight
/N/slate/jindjia/Btools/miniforge3/envs/megatron-TE/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/N/slate/jindjia/Btools/miniforge3/envs/megatron-TE/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
> learning rate decay style: cosine
WARNING: could not find the metadata file /N/scratch/jindjia/checkpoint/Course/hpc-course/Project/latest_checkpointed_iteration.txt 
    will not load any checkpoints and will start from random
(min, max) time across ranks (ms):
    load-checkpoint ................................: (1.03, 1.04)
[after model, optimizer, and learning rate scheduler are built] datetime: 2024-04-23 03:09:27 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      1280000
    validation: 128160
    test:       160
> building train, validation, and test datasets for GPT ...
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.969), (0.969, 0.999), (0.999, 1.0)]
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /N/scratch/jindjia/thepile/pile_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 134318121
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 134318121
INFO:megatron.core.datasets.gpt_dataset:Build and save the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Build and save the document index to ccbd5773b386e501db8a87de9e89aa97-GPTDataset-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Build and save the sample index to ccbd5773b386e501db8a87de9e89aa97-GPTDataset-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Build and save the shuffle index to ccbd5773b386e501db8a87de9e89aa97-GPTDataset-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 109444836
INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 1
INFO:megatron.core.datasets.gpt_dataset:Build and save the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Build and save the document index to b4321ed435e9ee1adf37ccf7acc7b347-GPTDataset-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Build and save the sample index to b4321ed435e9ee1adf37ccf7acc7b347-GPTDataset-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Build and save the shuffle index to b4321ed435e9ee1adf37ccf7acc7b347-GPTDataset-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 3488583
INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 1
INFO:megatron.core.datasets.gpt_dataset:Build and save the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Build and save the document index to 458e430da399bb8bf4ac9f1a2912461e-GPTDataset-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Build and save the sample index to 458e430da399bb8bf4ac9f1a2912461e-GPTDataset-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Build and save the shuffle index to 458e430da399bb8bf4ac9f1a2912461e-GPTDataset-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 115834
INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2024-04-23 03:09:43 
done with setup ...
training ...
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (236.52, 238.35)
    train/valid/test-data-iterators-setup ..........: (15624.53, 15647.83)
[before the start of training step] datetime: 2024-04-23 03:09:43 
 iteration       10/   80000 | consumed samples:          160 | elapsed time per iteration (ms): 978.7 | throughput per GPU (TFLOP/s/GPU): 45.6 | learning rate: 0.000E+00 | global batch size:    16 | loss scale: 8388608.0 | number of skipped iterations:  10 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (940.50, 940.55)
    forward-compute ................................: (405.23, 405.38)
    backward-compute ...............................: (535.01, 535.18)
    batch-generator ................................: (44.30, 53.04)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    all-grads-sync .................................: (21.21, 21.21)
    params-all-gather ..............................: (0.55, 0.55)
    optimizer-copy-to-main-grad ....................: (1.64, 2.24)
    optimizer-unscale-and-check-inf ................: (2.64, 2.65)
    optimizer ......................................: (10.38, 10.39)
 iteration       20/   80000 | consumed samples:          320 | elapsed time per iteration (ms): 767.0 | throughput per GPU (TFLOP/s/GPU): 58.2 | learning rate: 1.714E-06 | global batch size:    16 | lm loss: 1.091802E+01 | loss scale: 131072.0 | grad norm: 25.481 | number of skipped iterations:   6 | number of nan iterations:   0 |
[Rank 0] (after 20 iterations) memory (MB) | allocated: 5792.298828125 | max allocated: 26148.701171875 | reserved: 28932.0 | max reserved: 28932.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (734.14, 734.94)
    forward-compute ................................: (258.91, 259.07)
    backward-compute ...............................: (475.12, 475.77)
    batch-generator ................................: (1.58, 1.63)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (7.03, 7.04)
    params-all-gather ..............................: (3.58, 3.58)
    optimizer-copy-to-main-grad ....................: (1.62, 2.21)
    optimizer-unscale-and-check-inf ................: (1.50, 1.50)
    optimizer-clip-main-grad .......................: (4.13, 4.14)
    optimizer-count-zeros ..........................: (0.00, 0.00)
    optimizer-inner-step ...........................: (2.68, 2.72)
    optimizer-copy-main-to-model-params ............: (0.83, 0.98)
    optimizer ......................................: (18.86, 18.86)
 iteration       30/   80000 | consumed samples:          480 | elapsed time per iteration (ms): 773.1 | throughput per GPU (TFLOP/s/GPU): 57.7 | learning rate: 6.000E-06 | global batch size:    16 | lm loss: 9.641584E+00 | loss scale: 131072.0 | grad norm: 7.326 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (733.90, 735.00)
    forward-compute ................................: (258.84, 258.97)
    backward-compute ...............................: (474.96, 475.92)
    batch-generator ................................: (1.56, 1.61)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (7.04, 7.05)
    params-all-gather ..............................: (8.12, 8.12)
    optimizer-copy-to-main-grad ....................: (1.60, 2.21)
    optimizer-unscale-and-check-inf ................: (1.49, 1.50)
    optimizer-clip-main-grad .......................: (2.40, 2.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.18, 4.23)
    optimizer-copy-main-to-model-params ............: (2.06, 2.34)
    optimizer ......................................: (24.66, 24.67)
 iteration       40/   80000 | consumed samples:          640 | elapsed time per iteration (ms): 772.5 | throughput per GPU (TFLOP/s/GPU): 57.8 | learning rate: 1.029E-05 | global batch size:    16 | lm loss: 9.242706E+00 | loss scale: 131072.0 | grad norm: 3.286 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (733.12, 734.56)
    forward-compute ................................: (258.86, 259.00)
    backward-compute ...............................: (474.16, 475.45)
    batch-generator ................................: (1.54, 1.61)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (7.02, 7.05)
    params-all-gather ..............................: (8.12, 8.12)
    optimizer-copy-to-main-grad ....................: (1.60, 2.22)
    optimizer-unscale-and-check-inf ................: (1.49, 1.49)
    optimizer-clip-main-grad .......................: (2.40, 2.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.18, 4.24)
    optimizer-copy-main-to-model-params ............: (2.06, 2.34)
    optimizer ......................................: (24.66, 24.67)
 iteration       50/   80000 | consumed samples:          800 | elapsed time per iteration (ms): 772.9 | throughput per GPU (TFLOP/s/GPU): 57.8 | learning rate: 1.457E-05 | global batch size:    16 | lm loss: 8.668694E+00 | loss scale: 131072.0 | grad norm: 2.608 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (734.08, 735.00)
    forward-compute ................................: (258.76, 258.89)
    backward-compute ...............................: (475.24, 476.00)
    batch-generator ................................: (1.55, 1.59)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (7.04, 7.05)
    params-all-gather ..............................: (8.11, 8.11)
    optimizer-copy-to-main-grad ....................: (1.60, 2.21)
    optimizer-unscale-and-check-inf ................: (1.49, 1.50)
    optimizer-clip-main-grad .......................: (2.39, 2.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.17, 4.24)
    optimizer-copy-main-to-model-params ............: (2.06, 2.34)
    optimizer ......................................: (24.65, 24.66)
 iteration       60/   80000 | consumed samples:          960 | elapsed time per iteration (ms): 773.2 | throughput per GPU (TFLOP/s/GPU): 57.7 | learning rate: 1.886E-05 | global batch size:    16 | lm loss: 8.556458E+00 | loss scale: 131072.0 | grad norm: 2.495 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (733.77, 735.37)
    forward-compute ................................: (258.84, 258.98)
    backward-compute ...............................: (474.83, 476.30)
    batch-generator ................................: (1.54, 1.60)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (7.01, 7.03)
    params-all-gather ..............................: (8.12, 8.12)
    optimizer-copy-to-main-grad ....................: (1.60, 2.20)
    optimizer-unscale-and-check-inf ................: (1.49, 1.49)
    optimizer-clip-main-grad .......................: (2.39, 2.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.18, 4.23)
    optimizer-copy-main-to-model-params ............: (2.06, 2.34)
    optimizer ......................................: (24.66, 24.67)
 iteration       70/   80000 | consumed samples:         1120 | elapsed time per iteration (ms): 772.8 | throughput per GPU (TFLOP/s/GPU): 57.8 | learning rate: 2.314E-05 | global batch size:    16 | lm loss: 8.490298E+00 | loss scale: 131072.0 | grad norm: 2.260 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (734.06, 734.71)
    forward-compute ................................: (258.94, 259.07)
    backward-compute ...............................: (475.03, 475.54)
    batch-generator ................................: (1.56, 1.60)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (7.02, 7.03)
    params-all-gather ..............................: (8.12, 8.12)
    optimizer-copy-to-main-grad ....................: (1.60, 2.20)
    optimizer-unscale-and-check-inf ................: (1.49, 1.50)
    optimizer-clip-main-grad .......................: (2.39, 2.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.18, 4.24)
    optimizer-copy-main-to-model-params ............: (2.06, 2.34)
    optimizer ......................................: (24.63, 24.65)
 iteration       80/   80000 | consumed samples:         1280 | elapsed time per iteration (ms): 772.7 | throughput per GPU (TFLOP/s/GPU): 57.8 | learning rate: 2.743E-05 | global batch size:    16 | lm loss: 8.239981E+00 | loss scale: 131072.0 | grad norm: 2.944 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (734.00, 734.77)
    forward-compute ................................: (258.88, 259.02)
    backward-compute ...............................: (475.03, 475.65)
    batch-generator ................................: (1.54, 1.59)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (7.00, 7.01)
    params-all-gather ..............................: (8.11, 8.12)
    optimizer-copy-to-main-grad ....................: (1.60, 2.20)
    optimizer-unscale-and-check-inf ................: (1.49, 1.49)
    optimizer-clip-main-grad .......................: (2.40, 2.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.18, 4.24)
    optimizer-copy-main-to-model-params ............: (2.06, 2.34)
    optimizer ......................................: (24.62, 24.63)
 iteration       90/   80000 | consumed samples:         1440 | elapsed time per iteration (ms): 773.1 | throughput per GPU (TFLOP/s/GPU): 57.7 | learning rate: 3.171E-05 | global batch size:    16 | lm loss: 7.969742E+00 | loss scale: 131072.0 | grad norm: 2.537 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (734.21, 735.31)
    forward-compute ................................: (259.02, 259.17)
    backward-compute ...............................: (475.10, 476.04)
    batch-generator ................................: (1.56, 1.59)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (6.99, 7.01)
    params-all-gather ..............................: (8.13, 8.13)
    optimizer-copy-to-main-grad ....................: (1.60, 2.19)
    optimizer-unscale-and-check-inf ................: (1.49, 1.49)
    optimizer-clip-main-grad .......................: (2.39, 2.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.17, 4.23)
    optimizer-copy-main-to-model-params ............: (2.06, 2.34)
    optimizer ......................................: (24.63, 24.64)
 iteration      100/   80000 | consumed samples:         1600 | elapsed time per iteration (ms): 772.9 | throughput per GPU (TFLOP/s/GPU): 57.8 | learning rate: 3.600E-05 | global batch size:    16 | lm loss: 7.694310E+00 | loss scale: 131072.0 | grad norm: 7.215 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (734.13, 735.72)
    forward-compute ................................: (258.93, 259.07)
    backward-compute ...............................: (475.10, 476.56)
    batch-generator ................................: (1.56, 1.61)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (6.94, 6.95)
    params-all-gather ..............................: (8.12, 8.13)
    optimizer-copy-to-main-grad ....................: (1.58, 2.11)
    optimizer-unscale-and-check-inf ................: (1.46, 1.46)
    optimizer-clip-main-grad .......................: (2.38, 2.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.17, 4.23)
    optimizer-copy-main-to-model-params ............: (2.06, 2.33)
    optimizer ......................................: (24.41, 24.42)
(min, max) time across ranks (ms):
    evaluate .......................................: (2223.29, 2223.50)
-----------------------------------------------------------------------------------------------
 validation loss at iteration 100 | lm loss value: 7.682127E+00 | lm loss PPL: 2.169230E+03 | 
-----------------------------------------------------------------------------------------------
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 2752669.0 ON nid0703 CANCELLED AT 2024-04-23T03:11:06 ***
++ copy_files
++ mkdir -p /N/slate/jindjia/bash_scripts/Course/hpc-course/Project/2752669/wandb
slurmstepd: error: *** JOB 2752669 ON nid0703 CANCELLED AT 2024-04-23T03:11:06 ***
