no change     /N/slate/jindjia/Btools/miniforge3/condabin/conda
no change     /N/slate/jindjia/Btools/miniforge3/bin/conda
no change     /N/slate/jindjia/Btools/miniforge3/bin/conda-env
no change     /N/slate/jindjia/Btools/miniforge3/bin/activate
no change     /N/slate/jindjia/Btools/miniforge3/bin/deactivate
no change     /N/slate/jindjia/Btools/miniforge3/etc/profile.d/conda.sh
no change     /N/slate/jindjia/Btools/miniforge3/etc/fish/conf.d/conda.fish
no change     /N/slate/jindjia/Btools/miniforge3/shell/condabin/Conda.psm1
no change     /N/slate/jindjia/Btools/miniforge3/shell/condabin/conda-hook.ps1
no change     /N/slate/jindjia/Btools/miniforge3/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /N/slate/jindjia/Btools/miniforge3/etc/profile.d/conda.csh
modified      /N/u/jindjia/BigRed200/.bashrc

==> For changes to take effect, close and re-open your current shell. <==

Added mamba to /N/u/jindjia/BigRed200/.bashrc

==> For changes to take effect, close and re-open your current shell. <==

++ scontrol show hostnames
++ sort -u
+ HOSTNAMES='nid0703
nid0704'
+ HOSTLIST=
+ FIRST_HOST=
+ for HOST in $HOSTNAMES
+ '[' -z '' ']'
+ FIRST_HOST=nid0703
+ HOST_ARRAY+=($HOST)
+ HOSTLIST=nid0703,
+ for HOST in $HOSTNAMES
+ '[' -z nid0703 ']'
+ HOST_ARRAY+=($HOST)
+ HOSTLIST=nid0703,nid0704,
+ HOSTLIST=nid0703,nid0704
+ MASTER_ADDR=nid0703
+ srun --nodes=2 --gres=gpu:4 --export=ALL,GPUS_PER_NODE=4,MASTER_ADDR=nid0703,NNODES=2 bash -c ./run_dist.sh
+ MASTER_PORT=6000
+ WORLD_SIZE=8
+ NODE_RANK=0
+ VOCAB_FILE=/N/scratch/jindjia/thepile/vocab.json
+ MERGE_FILE=/N/scratch/jindjia/thepile/merges.txt
+ DATA_PATH=/N/scratch/jindjia/thepile/pile_text_document
+ CHECKPOINT_PATH=/N/scratch/jindjia/checkpoint/Course/hpc-course/Project
+ WANDB_DIR=/tmp/2752660/wandb
+ TENSORBOARD_DIR=/tmp/2752660/tensorboard
+ trap copy_files EXIT
+ DISTRIBUTED_ARGS='
    --nproc_per_node 4     --nnodes 2     --node_rank 0     --master_addr nid0703     --master_port 6000 '
+ MODEL_ARGS='
    --num-layers 24     --hidden-size 1024     --num-attention-heads 16     --seq-length 2048     --max-position-embeddings 2048 '
+ TRAINING_ARGS='
    --tensor-model-parallel-size 1     --pipeline-model-parallel-size 1     --micro-batch-size 8     --global-batch-size 64     --train-iters 80000 '
+ OPTIMIZER_ARGS='
    --lr 0.0003     --lr-decay-iters 70000     --lr-decay-style cosine     --min-lr 0.00003     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-08     --weight-decay .1     --lr-warmup-fraction 0.01     --clip-grad 1.0     --loss-scale 0     --loss-scale-window 1000     --hysteresis 2     --min-loss-scale 1     --fp16     --use-distributed-optimizer '
+ DATA_ARGS='
    --data-path /N/scratch/jindjia/thepile/pile_text_document     --vocab-file /N/scratch/jindjia/thepile/vocab.json     --merge-file /N/scratch/jindjia/thepile/merges.txt     --data-cache-path /tmp/2752660/data_cache     --distributed-storage '
+ OUTPUT_ARGS='
    --log-interval 10     --timing-log-level 2     --log-timers-to-tensorboard     --tensorboard-dir /tmp/2752660/tensorboard     --tensorboard-log-interval 1     --save-interval 5002     --eval-interval 100     --eval-iters 10     --log-timers-to-tensorboard     --log-validation-ppl-to-tensorboard     --log-throughput     --wandb-project DEBUG     --wandb-save-dir /tmp/2752660/wandb     --wandb-exp-name 350M-baseline '
+ QUANTIZE_ARGS='
    --no-async-tensor-model-parallel-allreduce     --recompute-activations     --recompute-granularity selective '
+ export OMP_NUM_THREADS=16
+ OMP_NUM_THREADS=16
+ cd /N/slate/jindjia/LLM/Megatron-LM-Final-Design
+ torchrun --nproc_per_node 4 --nnodes 2 --node_rank 0 --master_addr nid0703 --master_port 6000 pretrain_gpt.py --num-layers 24 --hidden-size 1024 --num-attention-heads 16 --seq-length 2048 --max-position-embeddings 2048 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --micro-batch-size 8 --global-batch-size 64 --train-iters 80000 --lr 0.0003 --lr-decay-iters 70000 --lr-decay-style cosine --min-lr 0.00003 --adam-beta1 0.9 --adam-beta2 0.95 --adam-eps 1e-08 --weight-decay .1 --lr-warmup-fraction 0.01 --clip-grad 1.0 --loss-scale 0 --loss-scale-window 1000 --hysteresis 2 --min-loss-scale 1 --fp16 --use-distributed-optimizer --data-path /N/scratch/jindjia/thepile/pile_text_document --vocab-file /N/scratch/jindjia/thepile/vocab.json --merge-file /N/scratch/jindjia/thepile/merges.txt --data-cache-path /tmp/2752660/data_cache --distributed-storage --log-interval 10 --timing-log-level 2 --log-timers-to-tensorboard --tensorboard-dir /tmp/2752660/tensorboard --tensorboard-log-interval 1 --save-interval 5002 --eval-interval 100 --eval-iters 10 --log-timers-to-tensorboard --log-validation-ppl-to-tensorboard --log-throughput --wandb-project DEBUG --wandb-save-dir /tmp/2752660/wandb --wandb-exp-name 350M-baseline --no-async-tensor-model-parallel-allreduce --recompute-activations --recompute-granularity selective --distributed-backend nccl --save /N/scratch/jindjia/checkpoint/Course/hpc-course/Project --load /N/scratch/jindjia/checkpoint/Course/hpc-course/Project --exit-duration-in-mins 2840
+ MASTER_PORT=6000
+ WORLD_SIZE=8
+ NODE_RANK=1
+ VOCAB_FILE=/N/scratch/jindjia/thepile/vocab.json
+ MERGE_FILE=/N/scratch/jindjia/thepile/merges.txt
+ DATA_PATH=/N/scratch/jindjia/thepile/pile_text_document
+ CHECKPOINT_PATH=/N/scratch/jindjia/checkpoint/Course/hpc-course/Project
+ WANDB_DIR=/tmp/2752660/wandb
+ TENSORBOARD_DIR=/tmp/2752660/tensorboard
+ trap copy_files EXIT
+ DISTRIBUTED_ARGS='
    --nproc_per_node 4     --nnodes 2     --node_rank 1     --master_addr nid0703     --master_port 6000 '
+ MODEL_ARGS='
    --num-layers 24     --hidden-size 1024     --num-attention-heads 16     --seq-length 2048     --max-position-embeddings 2048 '
+ TRAINING_ARGS='
    --tensor-model-parallel-size 1     --pipeline-model-parallel-size 1     --micro-batch-size 8     --global-batch-size 64     --train-iters 80000 '
+ OPTIMIZER_ARGS='
    --lr 0.0003     --lr-decay-iters 70000     --lr-decay-style cosine     --min-lr 0.00003     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-08     --weight-decay .1     --lr-warmup-fraction 0.01     --clip-grad 1.0     --loss-scale 0     --loss-scale-window 1000     --hysteresis 2     --min-loss-scale 1     --fp16     --use-distributed-optimizer '
+ DATA_ARGS='
    --data-path /N/scratch/jindjia/thepile/pile_text_document     --vocab-file /N/scratch/jindjia/thepile/vocab.json     --merge-file /N/scratch/jindjia/thepile/merges.txt     --data-cache-path /tmp/2752660/data_cache     --distributed-storage '
+ OUTPUT_ARGS='
    --log-interval 10     --timing-log-level 2     --log-timers-to-tensorboard     --tensorboard-dir /tmp/2752660/tensorboard     --tensorboard-log-interval 1     --save-interval 5002     --eval-interval 100     --eval-iters 10     --log-timers-to-tensorboard     --log-validation-ppl-to-tensorboard     --log-throughput     --wandb-project DEBUG     --wandb-save-dir /tmp/2752660/wandb     --wandb-exp-name 350M-baseline '
+ QUANTIZE_ARGS='
    --no-async-tensor-model-parallel-allreduce     --recompute-activations     --recompute-granularity selective '
+ export OMP_NUM_THREADS=16
+ OMP_NUM_THREADS=16
+ cd /N/slate/jindjia/LLM/Megatron-LM-Final-Design
+ torchrun --nproc_per_node 4 --nnodes 2 --node_rank 1 --master_addr nid0703 --master_port 6000 pretrain_gpt.py --num-layers 24 --hidden-size 1024 --num-attention-heads 16 --seq-length 2048 --max-position-embeddings 2048 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --micro-batch-size 8 --global-batch-size 64 --train-iters 80000 --lr 0.0003 --lr-decay-iters 70000 --lr-decay-style cosine --min-lr 0.00003 --adam-beta1 0.9 --adam-beta2 0.95 --adam-eps 1e-08 --weight-decay .1 --lr-warmup-fraction 0.01 --clip-grad 1.0 --loss-scale 0 --loss-scale-window 1000 --hysteresis 2 --min-loss-scale 1 --fp16 --use-distributed-optimizer --data-path /N/scratch/jindjia/thepile/pile_text_document --vocab-file /N/scratch/jindjia/thepile/vocab.json --merge-file /N/scratch/jindjia/thepile/merges.txt --data-cache-path /tmp/2752660/data_cache --distributed-storage --log-interval 10 --timing-log-level 2 --log-timers-to-tensorboard --tensorboard-dir /tmp/2752660/tensorboard --tensorboard-log-interval 1 --save-interval 5002 --eval-interval 100 --eval-iters 10 --log-timers-to-tensorboard --log-validation-ppl-to-tensorboard --log-throughput --wandb-project DEBUG --wandb-save-dir /tmp/2752660/wandb --wandb-exp-name 350M-baseline --no-async-tensor-model-parallel-allreduce --recompute-activations --recompute-granularity selective --distributed-backend nccl --save /N/scratch/jindjia/checkpoint/Course/hpc-course/Project --load /N/scratch/jindjia/checkpoint/Course/hpc-course/Project --exit-duration-in-mins 2840
Zarr-based strategies will not be registered because of missing packages
using world size: 8, data-parallel size: 8, context-parallel size: 1 tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. True
  add_position_embedding .......................... True
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  apply_rope_fusion ............................... True
  async_tensor_model_parallel_allreduce ........... False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  bias_swiglu_fusion .............................. True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  check_for_nan_in_loss_and_grad .................. True
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  clone_scatter_output_in_embedding ............... True
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  context_parallel_size ........................... 1
  data_cache_path ................................. /tmp/2752660/data_cache
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 8
  data_path ....................................... ['/N/scratch/jindjia/thepile/pile_text_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  delay_grad_reduce ............................... True
  delay_param_gather .............................. False
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_storage ............................. True
  distributed_timeout_minutes ..................... 10
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  encoder_num_layers .............................. 24
  encoder_seq_length .............................. 2048
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... False
  eval_interval ................................... 100
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 2840
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_model_parallel_size ...................... 1
  ffn_hidden_size ................................. 4096
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 64
  gq_group_size_inter ............................. 128
  gq_group_size_intra ............................. 512
  gradient_accumulation_fusion .................... True
  gradient_quantization_bits_inter ................ 4
  gradient_quantization_bits_intra ................ 8
  gradients_quantization_start_iteration .......... 0
  group_query_attention ........................... False
  hadamard_transform .............................. False
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1024
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  kv_channels ..................................... 64
  lazy_mpu_init ................................... None
  load ............................................ /N/scratch/jindjia/checkpoint/Course/hpc-course/Project
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 10
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_progress .................................... False
  log_throughput .................................. True
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... 0.0
  loss_scale_window ............................... 1000.0
  lr .............................................. 0.0003
  lr_decay_iters .................................. 70000
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. 0.01
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  manual_gc ....................................... False
  manual_gc_eval .................................. True
  manual_gc_interval .............................. 0
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 2048
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... /N/scratch/jindjia/thepile/merges.txt
  micro_batch_size ................................ 8
  min_loss_scale .................................. 1.0
  min_lr .......................................... 3e-05
  moe_grouped_gemm ................................ False
  nccl_communicator_config_path ................... None
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... LayerNorm
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... None
  num_layers ...................................... 24
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 1
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  overlap_param_gather ............................ False
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 1
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... learned_absolute
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  quantized_gradients ............................. False
  quantized_weights ............................... False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... selective
  recompute_method ................................ None
  recompute_num_layers ............................ None
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_attention_gate ............................ 1
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_return_doc_ids ............................ False
  retro_verify_neighbor_count ..................... True
  retro_workdir ................................... None
  rotary_percent .................................. 1.0
  rotary_seq_len_interpolation_factor ............. None
  sample_rate ..................................... 1.0
  save ............................................ /N/scratch/jindjia/checkpoint/Course/hpc-course/Project
  save_interval ................................... 5002
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 2048
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  spec ............................................ None
  split ........................................... 969, 30, 1
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.1
  swiglu .......................................... False
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. /tmp/2752660/tensorboard
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  timing_log_level ................................ 2
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. None
  tokenizer_type .................................. GPT2BPETokenizer
  tp_comm_bulk_dgrad .............................. True
  tp_comm_bulk_wgrad .............................. True
  tp_comm_overlap ................................. False
  tp_comm_overlap_cfg ............................. None
  tp_comm_split_ag ................................ True
  tp_comm_split_rs ................................ True
  train_data_path ................................. None
  train_iters ..................................... 80000
  train_samples ................................... None
  transformer_impl ................................ local
  transformer_pipeline_model_parallel_size ........ 1
  untie_embeddings_and_output_weights ............. False
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_distributed_optimizer ....................... True
  use_flash_attn .................................. False
  use_mcore_models ................................ False
  use_one_sent_docs ............................... False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /N/scratch/jindjia/thepile/vocab.json
  vocab_size ...................................... None
  wandb_exp_name .................................. 350M-baseline
  wandb_project ................................... DEBUG
  wandb_save_dir .................................. /tmp/2752660/wandb
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  weight_quantization_bits ........................ 4
  world_size ...................................... 8
  wq_group_size ................................... 2048
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 1
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
2024-04-23 02:59:01.976694: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-04-23 02:59:01.976742: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-04-23 02:59:01.977855: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-04-23 02:59:02.009750: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-23 02:59:06.930789: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
> setting tensorboard ...
wandb: Currently logged in as: jiajinda001 (jinda-personal). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.2
wandb: Run data is saved locally in /tmp/2752660/wandb/wandb/run-20240423_025918-qx0jejvg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 350M-baseline
wandb: ⭐️ View project at https://wandb.ai/jinda-personal/DEBUG
wandb: 🚀 View run at https://wandb.ai/jinda-personal/DEBUG/runs/qx0jejvg
> initialized tensor model parallel with size 1
> initialized pipeline model parallel with size 1
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory '/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/core/datasets'
make: Nothing to be done for 'default'.
make: Leaving directory '/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 0.654 seconds
> compiling and loading fused kernels ...
>>> done with compiling and loading fused kernels. Compilation time: 2.084 seconds
/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
time to initialize megatron (seconds): 35.299
[after megatron is initialized] datetime: 2024-04-23 02:59:32 
building GPT model ...
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 355919872
INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (355919872 elements):
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.embedding.word_embeddings.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.embedding.position_embeddings.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.final_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.final_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.mlp.dense_h_to_4h.bias
/N/slate/jindjia/Btools/miniforge3/envs/megatron-TE/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/N/slate/jindjia/Btools/miniforge3/envs/megatron-TE/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/N/slate/jindjia/Btools/miniforge3/envs/megatron-TE/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/N/slate/jindjia/Btools/miniforge3/envs/megatron-TE/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
> learning rate decay style: cosine
/N/slate/jindjia/Btools/miniforge3/envs/megatron-TE/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/N/slate/jindjia/Btools/miniforge3/envs/megatron-TE/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/N/slate/jindjia/Btools/miniforge3/envs/megatron-TE/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/N/slate/jindjia/Btools/miniforge3/envs/megatron-TE/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
WARNING: could not find the metadata file /N/scratch/jindjia/checkpoint/Course/hpc-course/Project/latest_checkpointed_iteration.txt 
    will not load any checkpoints and will start from random
(min, max) time across ranks (ms):
    load-checkpoint ................................: (1.13, 1.14)
[after model, optimizer, and learning rate scheduler are built] datetime: 2024-04-23 02:59:33 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      5120000
    validation: 512640
    test:       640
> building train, validation, and test datasets for GPT ...
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.969), (0.969, 0.999), (0.999, 1.0)]
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /N/scratch/jindjia/thepile/pile_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 134318121
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 134318121
INFO:megatron.core.datasets.gpt_dataset:Build and save the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Build and save the document index to 8d4b72802798061891321960ed4555f8-GPTDataset-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Build and save the sample index to 8d4b72802798061891321960ed4555f8-GPTDataset-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Build and save the shuffle index to 8d4b72802798061891321960ed4555f8-GPTDataset-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 109444836
INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 1
INFO:megatron.core.datasets.gpt_dataset:Build and save the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Build and save the document index to ededbfeb5788f3e7192ffe225887dc07-GPTDataset-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Build and save the sample index to ededbfeb5788f3e7192ffe225887dc07-GPTDataset-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Build and save the shuffle index to ededbfeb5788f3e7192ffe225887dc07-GPTDataset-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 3488583
INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 1
INFO:megatron.core.datasets.gpt_dataset:Build and save the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Build and save the document index to 68668ed5ed3cd61231e6f9bf91be06e9-GPTDataset-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Build and save the sample index to 68668ed5ed3cd61231e6f9bf91be06e9-GPTDataset-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Build and save the shuffle index to 68668ed5ed3cd61231e6f9bf91be06e9-GPTDataset-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 115834
INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2024-04-23 02:59:49 
done with setup ...
training ...
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (337.30, 492.75)
    train/valid/test-data-iterators-setup ..........: (16033.71, 16073.38)
[before the start of training step] datetime: 2024-04-23 02:59:49 
 iteration       10/   80000 | consumed samples:          640 | elapsed time per iteration (ms): 1203.4 | throughput per GPU (TFLOP/s/GPU): 37.1 | learning rate: 0.000E+00 | global batch size:    64 | loss scale: 8388608.0 | number of skipped iterations:  10 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (1045.15, 1054.74)
    forward-compute ................................: (538.25, 538.37)
    backward-compute ...............................: (506.68, 516.24)
    batch-generator ................................: (75.36, 140.48)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    all-grads-sync .................................: (121.81, 122.11)
    params-all-gather ..............................: (0.55, 0.55)
    optimizer-copy-to-main-grad ....................: (0.29, 0.76)
    optimizer-unscale-and-check-inf ................: (1.38, 1.39)
    optimizer ......................................: (17.96, 17.97)
 iteration       20/   80000 | consumed samples:         1280 | elapsed time per iteration (ms): 848.6 | throughput per GPU (TFLOP/s/GPU): 52.6 | learning rate: 1.286E-06 | global batch size:    64 | lm loss: 1.095211E+01 | loss scale: 65536.0 | grad norm: 27.892 | number of skipped iterations:   7 | number of nan iterations:   0 |
[Rank 0] (after 20 iterations) memory (MB) | allocated: 3755.708984375 | max allocated: 24621.2587890625 | reserved: 28422.0 | max reserved: 28422.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (733.86, 736.59)
    forward-compute ................................: (261.02, 261.09)
    backward-compute ...............................: (472.72, 475.41)
    batch-generator ................................: (3.37, 3.60)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    all-grads-sync .................................: (62.78, 63.04)
    params-all-gather ..............................: (20.14, 20.25)
    optimizer-copy-to-main-grad ....................: (0.29, 0.70)
    optimizer-unscale-and-check-inf ................: (0.49, 0.50)
    optimizer-clip-main-grad .......................: (8.80, 12.33)
    optimizer-count-zeros ..........................: (0.00, 0.01)
    optimizer-inner-step ...........................: (1.24, 3.28)
    optimizer-copy-main-to-model-params ............: (0.23, 0.30)
    optimizer ......................................: (41.43, 41.54)
 iteration       30/   80000 | consumed samples:         1920 | elapsed time per iteration (ms): 882.1 | throughput per GPU (TFLOP/s/GPU): 50.6 | learning rate: 5.571E-06 | global batch size:    64 | lm loss: 9.842520E+00 | loss scale: 65536.0 | grad norm: 5.464 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (734.01, 737.08)
    forward-compute ................................: (260.96, 261.04)
    backward-compute ...............................: (472.93, 475.93)
    batch-generator ................................: (3.34, 3.56)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (62.87, 63.15)
    params-all-gather ..............................: (65.64, 65.93)
    optimizer-copy-to-main-grad ....................: (0.28, 0.73)
    optimizer-unscale-and-check-inf ................: (0.48, 0.49)
    optimizer-clip-main-grad .......................: (0.87, 0.89)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (1.08, 1.16)
    optimizer-copy-main-to-model-params ............: (0.77, 1.01)
    optimizer ......................................: (74.36, 74.65)
 iteration       40/   80000 | consumed samples:         2560 | elapsed time per iteration (ms): 881.6 | throughput per GPU (TFLOP/s/GPU): 50.6 | learning rate: 9.857E-06 | global batch size:    64 | lm loss: 9.023721E+00 | loss scale: 65536.0 | grad norm: 2.381 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (733.96, 736.24)
    forward-compute ................................: (260.72, 260.79)
    backward-compute ...............................: (473.12, 475.33)
    batch-generator ................................: (3.25, 3.55)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (62.68, 63.03)
    params-all-gather ..............................: (65.71, 66.02)
    optimizer-copy-to-main-grad ....................: (0.28, 0.71)
    optimizer-unscale-and-check-inf ................: (0.48, 0.49)
    optimizer-clip-main-grad .......................: (0.87, 0.89)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (1.08, 1.16)
    optimizer-copy-main-to-model-params ............: (0.77, 1.01)
    optimizer ......................................: (74.52, 74.84)
 iteration       50/   80000 | consumed samples:         3200 | elapsed time per iteration (ms): 881.9 | throughput per GPU (TFLOP/s/GPU): 50.6 | learning rate: 1.414E-05 | global batch size:    64 | lm loss: 8.843647E+00 | loss scale: 65536.0 | grad norm: 2.933 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (734.01, 736.31)
    forward-compute ................................: (260.54, 260.61)
    backward-compute ...............................: (473.35, 475.58)
    batch-generator ................................: (3.21, 3.51)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (62.74, 63.13)
    params-all-gather ..............................: (65.92, 66.21)
    optimizer-copy-to-main-grad ....................: (0.28, 0.70)
    optimizer-unscale-and-check-inf ................: (0.48, 0.49)
    optimizer-clip-main-grad .......................: (0.91, 0.92)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (1.08, 1.16)
    optimizer-copy-main-to-model-params ............: (0.77, 1.01)
    optimizer ......................................: (74.75, 75.04)
 iteration       60/   80000 | consumed samples:         3840 | elapsed time per iteration (ms): 881.7 | throughput per GPU (TFLOP/s/GPU): 50.6 | learning rate: 1.843E-05 | global batch size:    64 | lm loss: 8.678976E+00 | loss scale: 65536.0 | grad norm: 2.317 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (733.62, 736.41)
    forward-compute ................................: (260.66, 260.77)
    backward-compute ...............................: (472.84, 475.53)
    batch-generator ................................: (3.27, 3.41)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (62.59, 62.92)
    params-all-gather ..............................: (65.92, 66.27)
    optimizer-copy-to-main-grad ....................: (0.28, 0.75)
    optimizer-unscale-and-check-inf ................: (0.48, 0.49)
    optimizer-clip-main-grad .......................: (0.88, 0.90)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (1.08, 1.16)
    optimizer-copy-main-to-model-params ............: (0.77, 1.01)
    optimizer ......................................: (74.71, 75.06)
 iteration       70/   80000 | consumed samples:         4480 | elapsed time per iteration (ms): 881.7 | throughput per GPU (TFLOP/s/GPU): 50.6 | learning rate: 2.271E-05 | global batch size:    64 | lm loss: 8.298861E+00 | loss scale: 65536.0 | grad norm: 3.052 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (734.15, 736.29)
    forward-compute ................................: (260.81, 260.92)
    backward-compute ...............................: (473.21, 475.25)
    batch-generator ................................: (3.25, 3.42)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (62.52, 62.75)
    params-all-gather ..............................: (65.90, 66.27)
    optimizer-copy-to-main-grad ....................: (0.28, 0.71)
    optimizer-unscale-and-check-inf ................: (0.48, 0.49)
    optimizer-clip-main-grad .......................: (0.87, 0.88)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (1.08, 1.15)
    optimizer-copy-main-to-model-params ............: (0.77, 1.01)
    optimizer ......................................: (74.65, 75.03)
 iteration       80/   80000 | consumed samples:         5120 | elapsed time per iteration (ms): 881.1 | throughput per GPU (TFLOP/s/GPU): 50.7 | learning rate: 2.700E-05 | global batch size:    64 | lm loss: 8.120096E+00 | loss scale: 65536.0 | grad norm: 2.484 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (734.15, 736.79)
    forward-compute ................................: (260.80, 260.90)
    backward-compute ...............................: (473.23, 475.78)
    batch-generator ................................: (3.29, 3.51)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (62.52, 62.80)
    params-all-gather ..............................: (65.53, 65.88)
    optimizer-copy-to-main-grad ....................: (0.28, 0.71)
    optimizer-unscale-and-check-inf ................: (0.48, 0.49)
    optimizer-clip-main-grad .......................: (0.87, 0.89)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (1.08, 1.15)
    optimizer-copy-main-to-model-params ............: (0.77, 1.01)
    optimizer ......................................: (74.19, 74.54)
 iteration       90/   80000 | consumed samples:         5760 | elapsed time per iteration (ms): 881.8 | throughput per GPU (TFLOP/s/GPU): 50.6 | learning rate: 3.129E-05 | global batch size:    64 | lm loss: 7.751142E+00 | loss scale: 65536.0 | grad norm: 2.729 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (734.35, 737.07)
    forward-compute ................................: (260.87, 260.96)
    backward-compute ...............................: (473.36, 476.01)
    batch-generator ................................: (3.31, 3.63)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (62.55, 62.90)
    params-all-gather ..............................: (65.69, 66.08)
    optimizer-copy-to-main-grad ....................: (0.28, 0.72)
    optimizer-unscale-and-check-inf ................: (0.48, 0.49)
    optimizer-clip-main-grad .......................: (0.88, 0.89)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (1.08, 1.16)
    optimizer-copy-main-to-model-params ............: (0.77, 1.01)
    optimizer ......................................: (74.47, 74.87)
 iteration      100/   80000 | consumed samples:         6400 | elapsed time per iteration (ms): 881.3 | throughput per GPU (TFLOP/s/GPU): 50.7 | learning rate: 3.557E-05 | global batch size:    64 | lm loss: 7.696989E+00 | loss scale: 65536.0 | grad norm: 2.256 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (734.27, 736.56)
    forward-compute ................................: (260.72, 260.83)
    backward-compute ...............................: (473.44, 475.66)
    batch-generator ................................: (3.16, 3.59)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (62.54, 62.99)
    params-all-gather ..............................: (65.96, 66.24)
    optimizer-copy-to-main-grad ....................: (0.28, 0.70)
    optimizer-unscale-and-check-inf ................: (0.48, 0.48)
    optimizer-clip-main-grad .......................: (0.87, 0.88)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (1.09, 1.16)
    optimizer-copy-main-to-model-params ............: (0.77, 1.01)
    optimizer ......................................: (74.61, 74.88)
(min, max) time across ranks (ms):
    evaluate .......................................: (2268.99, 2269.09)
-----------------------------------------------------------------------------------------------
 validation loss at iteration 100 | lm loss value: 7.417736E+00 | lm loss PPL: 1.665258E+03 | 
-----------------------------------------------------------------------------------------------
 iteration      110/   80000 | consumed samples:         7040 | elapsed time per iteration (ms): 882.2 | throughput per GPU (TFLOP/s/GPU): 50.6 | learning rate: 3.986E-05 | global batch size:    64 | lm loss: 7.451733E+00 | loss scale: 65536.0 | grad norm: 1.811 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (734.64, 736.73)
    forward-compute ................................: (261.07, 261.15)
    backward-compute ...............................: (473.45, 475.47)
    batch-generator ................................: (6.70, 7.29)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (62.52, 62.78)
    params-all-gather ..............................: (65.85, 66.26)
    optimizer-copy-to-main-grad ....................: (0.28, 0.74)
    optimizer-unscale-and-check-inf ................: (0.49, 0.50)
    optimizer-clip-main-grad .......................: (0.88, 0.89)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (1.08, 1.16)
    optimizer-copy-main-to-model-params ............: (0.77, 1.01)
    optimizer ......................................: (74.68, 75.10)
 iteration      120/   80000 | consumed samples:         7680 | elapsed time per iteration (ms): 882.1 | throughput per GPU (TFLOP/s/GPU): 50.6 | learning rate: 4.414E-05 | global batch size:    64 | lm loss: 7.158170E+00 | loss scale: 65536.0 | grad norm: 2.571 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (734.75, 736.72)
    forward-compute ................................: (260.71, 260.83)
    backward-compute ...............................: (473.92, 475.77)
    batch-generator ................................: (3.25, 3.54)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    all-grads-sync .................................: (62.64, 62.99)
    params-all-gather ..............................: (65.92, 66.20)
    optimizer-copy-to-main-grad ....................: (0.28, 0.74)
    optimizer-unscale-and-check-inf ................: (0.48, 0.49)
    optimizer-clip-main-grad .......................: (0.88, 0.89)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (1.08, 1.16)
    optimizer-copy-main-to-model-params ............: (0.77, 1.01)
    optimizer ......................................: (74.73, 75.00)
 iteration      130/   80000 | consumed samples:         8320 | elapsed time per iteration (ms): 882.0 | throughput per GPU (TFLOP/s/GPU): 50.6 | learning rate: 4.843E-05 | global batch size:    64 | lm loss: 7.091727E+00 | loss scale: 65536.0 | grad norm: 2.475 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (734.32, 736.78)
    forward-compute ................................: (260.78, 260.88)
    backward-compute ...............................: (473.42, 475.79)
    batch-generator ................................: (3.27, 3.42)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (62.75, 63.08)
    params-all-gather ..............................: (65.54, 65.95)
    optimizer-copy-to-main-grad ....................: (0.28, 0.72)
    optimizer-unscale-and-check-inf ................: (0.49, 0.50)
    optimizer-clip-main-grad .......................: (0.89, 0.90)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (1.08, 1.16)
    optimizer-copy-main-to-model-params ............: (0.77, 1.01)
    optimizer ......................................: (74.42, 74.83)
 iteration      140/   80000 | consumed samples:         8960 | elapsed time per iteration (ms): 882.0 | throughput per GPU (TFLOP/s/GPU): 50.6 | learning rate: 5.271E-05 | global batch size:    64 | lm loss: 6.924879E+00 | loss scale: 65536.0 | grad norm: 1.838 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (734.47, 737.13)
    forward-compute ................................: (261.10, 261.22)
    backward-compute ...............................: (473.25, 475.80)
    batch-generator ................................: (3.48, 3.68)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (62.82, 63.14)
    params-all-gather ..............................: (65.57, 65.88)
    optimizer-copy-to-main-grad ....................: (0.29, 0.74)
    optimizer-unscale-and-check-inf ................: (0.49, 0.49)
    optimizer-clip-main-grad .......................: (0.91, 0.93)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (1.08, 1.16)
    optimizer-copy-main-to-model-params ............: (0.77, 1.01)
    optimizer ......................................: (74.39, 74.69)
 iteration      150/   80000 | consumed samples:         9600 | elapsed time per iteration (ms): 882.5 | throughput per GPU (TFLOP/s/GPU): 50.6 | learning rate: 5.700E-05 | global batch size:    64 | lm loss: 6.760665E+00 | loss scale: 65536.0 | grad norm: 1.556 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (734.44, 737.30)
    forward-compute ................................: (260.92, 261.01)
    backward-compute ...............................: (473.40, 476.18)
    batch-generator ................................: (3.31, 3.49)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (62.67, 63.08)
    params-all-gather ..............................: (65.73, 66.09)
    optimizer-copy-to-main-grad ....................: (0.28, 0.75)
    optimizer-unscale-and-check-inf ................: (0.48, 0.49)
    optimizer-clip-main-grad .......................: (0.87, 0.88)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (1.08, 1.15)
    optimizer-copy-main-to-model-params ............: (0.77, 1.01)
    optimizer ......................................: (74.42, 74.78)
 iteration      160/   80000 | consumed samples:        10240 | elapsed time per iteration (ms): 883.2 | throughput per GPU (TFLOP/s/GPU): 50.6 | learning rate: 6.129E-05 | global batch size:    64 | lm loss: 6.577200E+00 | loss scale: 65536.0 | grad norm: 2.764 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (734.43, 737.46)
    forward-compute ................................: (260.98, 261.06)
    backward-compute ...............................: (473.33, 476.29)
    batch-generator ................................: (3.31, 3.54)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (62.81, 63.04)
    params-all-gather ..............................: (65.90, 66.22)
    optimizer-copy-to-main-grad ....................: (0.28, 0.75)
    optimizer-unscale-and-check-inf ................: (0.49, 0.50)
    optimizer-clip-main-grad .......................: (0.88, 0.89)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (1.08, 1.15)
    optimizer-copy-main-to-model-params ............: (0.77, 1.01)
    optimizer ......................................: (74.75, 75.07)
 iteration      170/   80000 | consumed samples:        10880 | elapsed time per iteration (ms): 882.1 | throughput per GPU (TFLOP/s/GPU): 50.6 | learning rate: 6.557E-05 | global batch size:    64 | lm loss: 6.561514E+00 | loss scale: 65536.0 | grad norm: 1.979 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (734.55, 736.62)
    forward-compute ................................: (260.84, 260.96)
    backward-compute ...............................: (473.59, 475.55)
    batch-generator ................................: (3.37, 3.52)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (62.78, 63.05)
    params-all-gather ..............................: (65.79, 66.21)
    optimizer-copy-to-main-grad ....................: (0.28, 0.70)
    optimizer-unscale-and-check-inf ................: (0.48, 0.49)
    optimizer-clip-main-grad .......................: (0.88, 0.89)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (1.08, 1.15)
    optimizer-copy-main-to-model-params ............: (0.77, 1.01)
    optimizer ......................................: (74.61, 75.04)
 iteration      180/   80000 | consumed samples:        11520 | elapsed time per iteration (ms): 882.7 | throughput per GPU (TFLOP/s/GPU): 50.6 | learning rate: 6.986E-05 | global batch size:    64 | lm loss: 6.460475E+00 | loss scale: 65536.0 | grad norm: 2.570 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (734.59, 736.90)
    forward-compute ................................: (260.90, 261.00)
    backward-compute ...............................: (473.57, 475.78)
    batch-generator ................................: (3.28, 3.61)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    all-grads-sync .................................: (62.90, 63.27)
    params-all-gather ..............................: (65.73, 66.10)
    optimizer-copy-to-main-grad ....................: (0.28, 0.71)
    optimizer-unscale-and-check-inf ................: (0.48, 0.49)
    optimizer-clip-main-grad .......................: (0.89, 0.90)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (1.08, 1.15)
    optimizer-copy-main-to-model-params ............: (0.77, 1.01)
    optimizer ......................................: (74.49, 74.87)
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 2752660.0 ON nid0703 CANCELLED AT 2024-04-23T03:02:38 ***
++ copy_files
++ copy_files
++ mkdir -p /N/slate/jindjia/bash_scripts/Course/hpc-course/Project/2752660/wandb
++ mkdir -p /N/slate/jindjia/bash_scripts/Course/hpc-course/Project/2752660/wandb
slurmstepd: error: *** JOB 2752660 ON nid0703 CANCELLED AT 2024-04-23T03:02:38 ***
