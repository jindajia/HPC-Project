no change     /N/slate/jindjia/Btools/miniforge3/condabin/conda
no change     /N/slate/jindjia/Btools/miniforge3/bin/conda
no change     /N/slate/jindjia/Btools/miniforge3/bin/conda-env
no change     /N/slate/jindjia/Btools/miniforge3/bin/activate
no change     /N/slate/jindjia/Btools/miniforge3/bin/deactivate
no change     /N/slate/jindjia/Btools/miniforge3/etc/profile.d/conda.sh
no change     /N/slate/jindjia/Btools/miniforge3/etc/fish/conf.d/conda.fish
no change     /N/slate/jindjia/Btools/miniforge3/shell/condabin/Conda.psm1
no change     /N/slate/jindjia/Btools/miniforge3/shell/condabin/conda-hook.ps1
no change     /N/slate/jindjia/Btools/miniforge3/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /N/slate/jindjia/Btools/miniforge3/etc/profile.d/conda.csh
modified      /N/u/jindjia/BigRed200/.bashrc

==> For changes to take effect, close and re-open your current shell. <==

Added mamba to /N/u/jindjia/BigRed200/.bashrc

==> For changes to take effect, close and re-open your current shell. <==

++ scontrol show hostnames
++ sort -u
+ HOSTNAMES='nid0703
nid0704'
+ HOSTLIST=
+ FIRST_HOST=
+ for HOST in $HOSTNAMES
+ '[' -z '' ']'
+ FIRST_HOST=nid0703
+ HOST_ARRAY+=($HOST)
+ HOSTLIST=nid0703,
+ for HOST in $HOSTNAMES
+ '[' -z nid0703 ']'
+ HOST_ARRAY+=($HOST)
+ HOSTLIST=nid0703,nid0704,
+ HOSTLIST=nid0703,nid0704
+ MASTER_ADDR=nid0703
+ srun --nodes=2 --gres=gpu:4 --export=ALL,GPUS_PER_NODE=4,MASTER_ADDR=nid0703,NNODES=2 bash -c ./run_dist.sh
+ MASTER_PORT=6000
+ WORLD_SIZE=8
+ NODE_RANK=0
+ VOCAB_FILE=/N/scratch/jindjia/thepile/vocab.json
+ MERGE_FILE=/N/scratch/jindjia/thepile/merges.txt
+ DATA_PATH=/N/scratch/jindjia/thepile/pile_text_document
+ CHECKPOINT_PATH=/N/scratch/jindjia/checkpoint/Course/hpc-course/Project
+ WANDB_DIR=/tmp/2752698/wandb
+ TENSORBOARD_DIR=/tmp/2752698/tensorboard
+ trap copy_files EXIT
+ DISTRIBUTED_ARGS='
    --nproc_per_node 4     --nnodes 2     --node_rank 0     --master_addr nid0703     --master_port 6000 '
+ MODEL_ARGS='
    --num-layers 24     --hidden-size 2048     --num-attention-heads 16     --seq-length 2048     --max-position-embeddings 2048 '
+ TRAINING_ARGS='
    --tensor-model-parallel-size 1     --pipeline-model-parallel-size 1     --micro-batch-size 2     --global-batch-size 16     --train-iters 80000 '
+ OPTIMIZER_ARGS='
    --lr 0.0003     --lr-decay-iters 70000     --lr-decay-style cosine     --min-lr 0.00003     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-08     --weight-decay .1     --lr-warmup-fraction 0.01     --clip-grad 1.0     --loss-scale 0     --loss-scale-window 1000     --hysteresis 2     --min-loss-scale 1     --fp16     --use-distributed-optimizer '
+ DATA_ARGS='
    --data-path /N/scratch/jindjia/thepile/pile_text_document     --vocab-file /N/scratch/jindjia/thepile/vocab.json     --merge-file /N/scratch/jindjia/thepile/merges.txt     --data-cache-path /tmp/2752698/data_cache     --distributed-storage '
+ OUTPUT_ARGS='
    --log-interval 10     --timing-log-level 2     --log-timers-to-tensorboard     --tensorboard-dir /tmp/2752698/tensorboard     --tensorboard-log-interval 1     --save-interval 5002     --eval-interval 100     --eval-iters 10     --log-timers-to-tensorboard     --log-validation-ppl-to-tensorboard     --log-throughput     --wandb-project DEBUG     --wandb-save-dir /tmp/2752698/wandb     --wandb-exp-name 350M-baseline '
+ QUANTIZE_ARGS='
    --no-async-tensor-model-parallel-allreduce     --recompute-activations     --recompute-granularity selective '
+ export OMP_NUM_THREADS=16
+ OMP_NUM_THREADS=16
+ cd /N/slate/jindjia/LLM/Megatron-LM-Final-Design
+ torchrun --nproc_per_node 4 --nnodes 2 --node_rank 0 --master_addr nid0703 --master_port 6000 pretrain_gpt.py --num-layers 24 --hidden-size 2048 --num-attention-heads 16 --seq-length 2048 --max-position-embeddings 2048 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --micro-batch-size 2 --global-batch-size 16 --train-iters 80000 --lr 0.0003 --lr-decay-iters 70000 --lr-decay-style cosine --min-lr 0.00003 --adam-beta1 0.9 --adam-beta2 0.95 --adam-eps 1e-08 --weight-decay .1 --lr-warmup-fraction 0.01 --clip-grad 1.0 --loss-scale 0 --loss-scale-window 1000 --hysteresis 2 --min-loss-scale 1 --fp16 --use-distributed-optimizer --data-path /N/scratch/jindjia/thepile/pile_text_document --vocab-file /N/scratch/jindjia/thepile/vocab.json --merge-file /N/scratch/jindjia/thepile/merges.txt --data-cache-path /tmp/2752698/data_cache --distributed-storage --log-interval 10 --timing-log-level 2 --log-timers-to-tensorboard --tensorboard-dir /tmp/2752698/tensorboard --tensorboard-log-interval 1 --save-interval 5002 --eval-interval 100 --eval-iters 10 --log-timers-to-tensorboard --log-validation-ppl-to-tensorboard --log-throughput --wandb-project DEBUG --wandb-save-dir /tmp/2752698/wandb --wandb-exp-name 350M-baseline --no-async-tensor-model-parallel-allreduce --recompute-activations --recompute-granularity selective --distributed-backend nccl --save /N/scratch/jindjia/checkpoint/Course/hpc-course/Project --load /N/scratch/jindjia/checkpoint/Course/hpc-course/Project --exit-duration-in-mins 2840
+ MASTER_PORT=6000
+ WORLD_SIZE=8
+ NODE_RANK=1
+ VOCAB_FILE=/N/scratch/jindjia/thepile/vocab.json
+ MERGE_FILE=/N/scratch/jindjia/thepile/merges.txt
+ DATA_PATH=/N/scratch/jindjia/thepile/pile_text_document
+ CHECKPOINT_PATH=/N/scratch/jindjia/checkpoint/Course/hpc-course/Project
+ WANDB_DIR=/tmp/2752698/wandb
+ TENSORBOARD_DIR=/tmp/2752698/tensorboard
+ trap copy_files EXIT
+ DISTRIBUTED_ARGS='
    --nproc_per_node 4     --nnodes 2     --node_rank 1     --master_addr nid0703     --master_port 6000 '
+ MODEL_ARGS='
    --num-layers 24     --hidden-size 2048     --num-attention-heads 16     --seq-length 2048     --max-position-embeddings 2048 '
+ TRAINING_ARGS='
    --tensor-model-parallel-size 1     --pipeline-model-parallel-size 1     --micro-batch-size 2     --global-batch-size 16     --train-iters 80000 '
+ OPTIMIZER_ARGS='
    --lr 0.0003     --lr-decay-iters 70000     --lr-decay-style cosine     --min-lr 0.00003     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-08     --weight-decay .1     --lr-warmup-fraction 0.01     --clip-grad 1.0     --loss-scale 0     --loss-scale-window 1000     --hysteresis 2     --min-loss-scale 1     --fp16     --use-distributed-optimizer '
+ DATA_ARGS='
    --data-path /N/scratch/jindjia/thepile/pile_text_document     --vocab-file /N/scratch/jindjia/thepile/vocab.json     --merge-file /N/scratch/jindjia/thepile/merges.txt     --data-cache-path /tmp/2752698/data_cache     --distributed-storage '
+ OUTPUT_ARGS='
    --log-interval 10     --timing-log-level 2     --log-timers-to-tensorboard     --tensorboard-dir /tmp/2752698/tensorboard     --tensorboard-log-interval 1     --save-interval 5002     --eval-interval 100     --eval-iters 10     --log-timers-to-tensorboard     --log-validation-ppl-to-tensorboard     --log-throughput     --wandb-project DEBUG     --wandb-save-dir /tmp/2752698/wandb     --wandb-exp-name 350M-baseline '
+ QUANTIZE_ARGS='
    --no-async-tensor-model-parallel-allreduce     --recompute-activations     --recompute-granularity selective '
+ export OMP_NUM_THREADS=16
+ OMP_NUM_THREADS=16
+ cd /N/slate/jindjia/LLM/Megatron-LM-Final-Design
+ torchrun --nproc_per_node 4 --nnodes 2 --node_rank 1 --master_addr nid0703 --master_port 6000 pretrain_gpt.py --num-layers 24 --hidden-size 2048 --num-attention-heads 16 --seq-length 2048 --max-position-embeddings 2048 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --micro-batch-size 2 --global-batch-size 16 --train-iters 80000 --lr 0.0003 --lr-decay-iters 70000 --lr-decay-style cosine --min-lr 0.00003 --adam-beta1 0.9 --adam-beta2 0.95 --adam-eps 1e-08 --weight-decay .1 --lr-warmup-fraction 0.01 --clip-grad 1.0 --loss-scale 0 --loss-scale-window 1000 --hysteresis 2 --min-loss-scale 1 --fp16 --use-distributed-optimizer --data-path /N/scratch/jindjia/thepile/pile_text_document --vocab-file /N/scratch/jindjia/thepile/vocab.json --merge-file /N/scratch/jindjia/thepile/merges.txt --data-cache-path /tmp/2752698/data_cache --distributed-storage --log-interval 10 --timing-log-level 2 --log-timers-to-tensorboard --tensorboard-dir /tmp/2752698/tensorboard --tensorboard-log-interval 1 --save-interval 5002 --eval-interval 100 --eval-iters 10 --log-timers-to-tensorboard --log-validation-ppl-to-tensorboard --log-throughput --wandb-project DEBUG --wandb-save-dir /tmp/2752698/wandb --wandb-exp-name 350M-baseline --no-async-tensor-model-parallel-allreduce --recompute-activations --recompute-granularity selective --distributed-backend nccl --save /N/scratch/jindjia/checkpoint/Course/hpc-course/Project --load /N/scratch/jindjia/checkpoint/Course/hpc-course/Project --exit-duration-in-mins 2840
Zarr-based strategies will not be registered because of missing packages
using world size: 8, data-parallel size: 8, context-parallel size: 1 tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. True
  add_position_embedding .......................... True
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  apply_rope_fusion ............................... True
  async_tensor_model_parallel_allreduce ........... False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  bias_swiglu_fusion .............................. True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  check_for_nan_in_loss_and_grad .................. True
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  clone_scatter_output_in_embedding ............... True
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  context_parallel_size ........................... 1
  data_cache_path ................................. /tmp/2752698/data_cache
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 8
  data_path ....................................... ['/N/scratch/jindjia/thepile/pile_text_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  delay_grad_reduce ............................... True
  delay_param_gather .............................. False
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_storage ............................. True
  distributed_timeout_minutes ..................... 10
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  encoder_num_layers .............................. 24
  encoder_seq_length .............................. 2048
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... False
  eval_interval ................................... 100
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 2840
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_model_parallel_size ...................... 1
  ffn_hidden_size ................................. 8192
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 16
  gq_group_size_inter ............................. 128
  gq_group_size_intra ............................. 512
  gradient_accumulation_fusion .................... True
  gradient_quantization_bits_inter ................ 4
  gradient_quantization_bits_intra ................ 8
  gradients_quantization_start_iteration .......... 0
  group_query_attention ........................... False
  hadamard_transform .............................. False
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 2048
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  kv_channels ..................................... 128
  lazy_mpu_init ................................... None
  load ............................................ /N/scratch/jindjia/checkpoint/Course/hpc-course/Project
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 10
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_progress .................................... False
  log_throughput .................................. True
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... 0.0
  loss_scale_window ............................... 1000.0
  lr .............................................. 0.0003
  lr_decay_iters .................................. 70000
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. 0.01
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  manual_gc ....................................... False
  manual_gc_eval .................................. True
  manual_gc_interval .............................. 0
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 2048
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... /N/scratch/jindjia/thepile/merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 3e-05
  moe_grouped_gemm ................................ False
  nccl_communicator_config_path ................... None
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... LayerNorm
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... None
  num_layers ...................................... 24
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 1
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  overlap_param_gather ............................ False
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 1
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... learned_absolute
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  quantized_gradients ............................. False
  quantized_weights ............................... False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... selective
  recompute_method ................................ None
  recompute_num_layers ............................ None
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_attention_gate ............................ 1
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_return_doc_ids ............................ False
  retro_verify_neighbor_count ..................... True
  retro_workdir ................................... None
  rotary_percent .................................. 1.0
  rotary_seq_len_interpolation_factor ............. None
  sample_rate ..................................... 1.0
  save ............................................ /N/scratch/jindjia/checkpoint/Course/hpc-course/Project
  save_interval ................................... 5002
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 2048
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  spec ............................................ None
  split ........................................... 969, 30, 1
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.1
  swiglu .......................................... False
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. /tmp/2752698/tensorboard
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  timing_log_level ................................ 2
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. None
  tokenizer_type .................................. GPT2BPETokenizer
  tp_comm_bulk_dgrad .............................. True
  tp_comm_bulk_wgrad .............................. True
  tp_comm_overlap ................................. False
  tp_comm_overlap_cfg ............................. None
  tp_comm_split_ag ................................ True
  tp_comm_split_rs ................................ True
  train_data_path ................................. None
  train_iters ..................................... 80000
  train_samples ................................... None
  transformer_impl ................................ local
  transformer_pipeline_model_parallel_size ........ 1
  untie_embeddings_and_output_weights ............. False
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_distributed_optimizer ....................... True
  use_flash_attn .................................. False
  use_mcore_models ................................ False
  use_one_sent_docs ............................... False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /N/scratch/jindjia/thepile/vocab.json
  vocab_size ...................................... None
  wandb_exp_name .................................. 350M-baseline
  wandb_project ................................... DEBUG
  wandb_save_dir .................................. /tmp/2752698/wandb
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  weight_quantization_bits ........................ 4
  world_size ...................................... 8
  wq_group_size ................................... 2048
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 1
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
2024-04-23 03:35:59.930929: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-04-23 03:35:59.930977: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-04-23 03:35:59.932097: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-04-23 03:35:59.959047: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-23 03:36:05.009724: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
> setting tensorboard ...
wandb: Currently logged in as: jiajinda001 (jinda-personal). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.2
wandb: Run data is saved locally in /tmp/2752698/wandb/wandb/run-20240423_033617-9nvn5y1q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 350M-baseline
wandb: ⭐️ View project at https://wandb.ai/jinda-personal/DEBUG
wandb: 🚀 View run at https://wandb.ai/jinda-personal/DEBUG/runs/9nvn5y1q
> initialized tensor model parallel with size 1
> initialized pipeline model parallel with size 1
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory '/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/core/datasets'
make: Nothing to be done for 'default'.
make: Leaving directory '/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 0.710 seconds
> compiling and loading fused kernels ...
>>> done with compiling and loading fused kernels. Compilation time: 2.040 seconds
/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/N/slate/jindjia/LLM/Megatron-LM-Final-Design/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
time to initialize megatron (seconds): 35.310
[after megatron is initialized] datetime: 2024-04-23 03:36:31 
building GPT model ...
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1315819520
/N/slate/jindjia/Btools/miniforge3/envs/megatron-TE/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/N/slate/jindjia/Btools/miniforge3/envs/megatron-TE/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/N/slate/jindjia/Btools/miniforge3/envs/megatron-TE/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/N/slate/jindjia/Btools/miniforge3/envs/megatron-TE/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1315819520 elements):
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.embedding.word_embeddings.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.final_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.final_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.embedding.position_embeddings.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.post_attention_norm.bias
/N/slate/jindjia/Btools/miniforge3/envs/megatron-TE/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight
/N/slate/jindjia/Btools/miniforge3/envs/megatron-TE/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/N/slate/jindjia/Btools/miniforge3/envs/megatron-TE/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/N/slate/jindjia/Btools/miniforge3/envs/megatron-TE/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
> learning rate decay style: cosine
WARNING: could not find the metadata file /N/scratch/jindjia/checkpoint/Course/hpc-course/Project/latest_checkpointed_iteration.txt 
    will not load any checkpoints and will start from random
(min, max) time across ranks (ms):
    load-checkpoint ................................: (1.19, 1.22)
[after model, optimizer, and learning rate scheduler are built] datetime: 2024-04-23 03:36:32 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      1280000
    validation: 128160
    test:       160
> building train, validation, and test datasets for GPT ...
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.969), (0.969, 0.999), (0.999, 1.0)]
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /N/scratch/jindjia/thepile/pile_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 134318121
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 134318121
INFO:megatron.core.datasets.gpt_dataset:Build and save the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Build and save the document index to ccbd5773b386e501db8a87de9e89aa97-GPTDataset-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Build and save the sample index to ccbd5773b386e501db8a87de9e89aa97-GPTDataset-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Build and save the shuffle index to ccbd5773b386e501db8a87de9e89aa97-GPTDataset-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 109444836
INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 1
INFO:megatron.core.datasets.gpt_dataset:Build and save the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Build and save the document index to b4321ed435e9ee1adf37ccf7acc7b347-GPTDataset-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Build and save the sample index to b4321ed435e9ee1adf37ccf7acc7b347-GPTDataset-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Build and save the shuffle index to b4321ed435e9ee1adf37ccf7acc7b347-GPTDataset-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 3488583
INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 1
INFO:megatron.core.datasets.gpt_dataset:Build and save the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Build and save the document index to 458e430da399bb8bf4ac9f1a2912461e-GPTDataset-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Build and save the sample index to 458e430da399bb8bf4ac9f1a2912461e-GPTDataset-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Build and save the shuffle index to 458e430da399bb8bf4ac9f1a2912461e-GPTDataset-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 115834
INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2024-04-23 03:36:48 
done with setup ...
training ...
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (622.29, 663.19)
    train/valid/test-data-iterators-setup ..........: (15848.42, 15941.85)
[before the start of training step] datetime: 2024-04-23 03:36:48 
 iteration       10/   80000 | consumed samples:          160 | elapsed time per iteration (ms): 851.0 | throughput per GPU (TFLOP/s/GPU): 43.7 | learning rate: 0.000E+00 | global batch size:    16 | loss scale: 8388608.0 | number of skipped iterations:  10 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (520.27, 526.12)
    forward-compute ................................: (266.54, 266.91)
    backward-compute ...............................: (253.50, 259.08)
    batch-generator ................................: (20.89, 24.49)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    all-grads-sync .................................: (292.91, 292.98)
    params-all-gather ..............................: (1.94, 1.95)
    optimizer-copy-to-main-grad ....................: (1.00, 1.14)
    optimizer-unscale-and-check-inf ................: (8.88, 8.89)
    optimizer ......................................: (23.58, 23.59)
 iteration       20/   80000 | consumed samples:          320 | elapsed time per iteration (ms): 622.6 | throughput per GPU (TFLOP/s/GPU): 59.7 | learning rate: 8.571E-07 | global batch size:    16 | lm loss: 1.115154E+01 | loss scale: 32768.0 | grad norm: 39.318 | number of skipped iterations:   8 | number of nan iterations:   0 |
[Rank 0] (after 20 iterations) memory (MB) | allocated: 10312.60546875 | max allocated: 18227.990234375 | reserved: 18568.0 | max reserved: 18568.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (317.74, 320.12)
    forward-compute ................................: (115.08, 115.17)
    backward-compute ...............................: (202.53, 204.85)
    batch-generator ................................: (0.92, 1.00)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (232.09, 232.17)
    params-all-gather ..............................: (49.81, 49.83)
    optimizer-copy-to-main-grad ....................: (1.00, 1.13)
    optimizer-unscale-and-check-inf ................: (1.27, 1.28)
    optimizer-clip-main-grad .......................: (3.38, 3.44)
    optimizer-count-zeros ..........................: (0.00, 0.00)
    optimizer-inner-step ...........................: (1.47, 1.55)
    optimizer-copy-main-to-model-params ............: (0.57, 0.60)
    optimizer ......................................: (63.35, 63.36)
 iteration       30/   80000 | consumed samples:          480 | elapsed time per iteration (ms): 817.3 | throughput per GPU (TFLOP/s/GPU): 45.5 | learning rate: 5.143E-06 | global batch size:    16 | lm loss: 9.686802E+00 | loss scale: 32768.0 | grad norm: 29.967 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (317.30, 319.97)
    forward-compute ................................: (114.96, 115.06)
    backward-compute ...............................: (202.23, 204.81)
    batch-generator ................................: (0.93, 0.98)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (232.24, 232.32)
    params-all-gather ..............................: (240.94, 241.06)
    optimizer-copy-to-main-grad ....................: (0.99, 1.13)
    optimizer-unscale-and-check-inf ................: (1.24, 1.25)
    optimizer-clip-main-grad .......................: (2.01, 2.02)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (3.79, 3.85)
    optimizer-copy-main-to-model-params ............: (2.86, 3.00)
    optimizer ......................................: (257.97, 258.09)
 iteration       40/   80000 | consumed samples:          640 | elapsed time per iteration (ms): 816.0 | throughput per GPU (TFLOP/s/GPU): 45.5 | learning rate: 9.429E-06 | global batch size:    16 | lm loss: 8.946727E+00 | loss scale: 32768.0 | grad norm: 6.496 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (317.23, 319.23)
    forward-compute ................................: (114.94, 115.02)
    backward-compute ...............................: (202.18, 204.10)
    batch-generator ................................: (0.91, 0.99)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (231.68, 231.76)
    params-all-gather ..............................: (240.69, 240.83)
    optimizer-copy-to-main-grad ....................: (0.99, 1.13)
    optimizer-unscale-and-check-inf ................: (1.23, 1.24)
    optimizer-clip-main-grad .......................: (2.02, 2.04)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (3.78, 3.85)
    optimizer-copy-main-to-model-params ............: (2.86, 3.00)
    optimizer ......................................: (257.69, 257.83)
 iteration       50/   80000 | consumed samples:          800 | elapsed time per iteration (ms): 818.4 | throughput per GPU (TFLOP/s/GPU): 45.4 | learning rate: 1.371E-05 | global batch size:    16 | lm loss: 8.237493E+00 | loss scale: 32768.0 | grad norm: 4.452 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (317.19, 319.41)
    forward-compute ................................: (114.91, 115.00)
    backward-compute ...............................: (202.17, 204.31)
    batch-generator ................................: (0.92, 0.98)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (233.93, 234.02)
    params-all-gather ..............................: (240.71, 240.80)
    optimizer-copy-to-main-grad ....................: (0.99, 1.13)
    optimizer-unscale-and-check-inf ................: (1.23, 1.24)
    optimizer-clip-main-grad .......................: (1.97, 1.99)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (3.78, 3.86)
    optimizer-copy-main-to-model-params ............: (2.86, 3.00)
    optimizer ......................................: (257.62, 257.73)
 iteration       60/   80000 | consumed samples:          960 | elapsed time per iteration (ms): 816.0 | throughput per GPU (TFLOP/s/GPU): 45.5 | learning rate: 1.800E-05 | global batch size:    16 | lm loss: 8.046667E+00 | loss scale: 32768.0 | grad norm: 4.200 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (317.32, 319.38)
    forward-compute ................................: (114.91, 114.97)
    backward-compute ...............................: (202.26, 204.32)
    batch-generator ................................: (0.92, 0.95)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (231.82, 231.91)
    params-all-gather ..............................: (240.79, 240.90)
    optimizer-copy-to-main-grad ....................: (0.99, 1.13)
    optimizer-unscale-and-check-inf ................: (1.24, 1.25)
    optimizer-clip-main-grad .......................: (1.99, 2.00)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (3.77, 3.85)
    optimizer-copy-main-to-model-params ............: (2.86, 3.00)
    optimizer ......................................: (257.75, 257.87)
 iteration       70/   80000 | consumed samples:         1120 | elapsed time per iteration (ms): 816.8 | throughput per GPU (TFLOP/s/GPU): 45.5 | learning rate: 2.229E-05 | global batch size:    16 | lm loss: 7.889898E+00 | loss scale: 32768.0 | grad norm: 3.182 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (317.25, 319.42)
    forward-compute ................................: (114.92, 115.03)
    backward-compute ...............................: (202.23, 204.29)
    batch-generator ................................: (0.90, 0.97)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (232.43, 232.49)
    params-all-gather ..............................: (240.75, 240.82)
    optimizer-copy-to-main-grad ....................: (0.99, 1.13)
    optimizer-unscale-and-check-inf ................: (1.23, 1.24)
    optimizer-clip-main-grad .......................: (1.97, 2.00)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (3.79, 3.85)
    optimizer-copy-main-to-model-params ............: (2.86, 3.00)
    optimizer ......................................: (257.63, 257.71)
 iteration       80/   80000 | consumed samples:         1280 | elapsed time per iteration (ms): 816.0 | throughput per GPU (TFLOP/s/GPU): 45.5 | learning rate: 2.657E-05 | global batch size:    16 | lm loss: 7.605988E+00 | loss scale: 32768.0 | grad norm: 3.326 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (317.34, 319.45)
    forward-compute ................................: (114.92, 115.02)
    backward-compute ...............................: (202.27, 204.32)
    batch-generator ................................: (0.91, 0.97)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (231.82, 231.89)
    params-all-gather ..............................: (240.52, 240.60)
    optimizer-copy-to-main-grad ....................: (0.99, 1.13)
    optimizer-unscale-and-check-inf ................: (1.23, 1.24)
    optimizer-clip-main-grad .......................: (1.98, 1.99)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (3.78, 3.86)
    optimizer-copy-main-to-model-params ............: (2.86, 3.00)
    optimizer ......................................: (257.41, 257.49)
 iteration       90/   80000 | consumed samples:         1440 | elapsed time per iteration (ms): 816.3 | throughput per GPU (TFLOP/s/GPU): 45.5 | learning rate: 3.086E-05 | global batch size:    16 | lm loss: 7.347008E+00 | loss scale: 32768.0 | grad norm: 4.185 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (317.36, 319.22)
    forward-compute ................................: (114.94, 115.03)
    backward-compute ...............................: (202.31, 204.10)
    batch-generator ................................: (0.91, 0.97)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (232.03, 232.09)
    params-all-gather ..............................: (240.78, 240.90)
    optimizer-copy-to-main-grad ....................: (0.99, 1.13)
    optimizer-unscale-and-check-inf ................: (1.24, 1.25)
    optimizer-clip-main-grad .......................: (2.01, 2.03)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (3.79, 3.86)
    optimizer-copy-main-to-model-params ............: (2.86, 3.00)
    optimizer ......................................: (257.85, 257.97)
 iteration      100/   80000 | consumed samples:         1600 | elapsed time per iteration (ms): 816.5 | throughput per GPU (TFLOP/s/GPU): 45.5 | learning rate: 3.514E-05 | global batch size:    16 | lm loss: 7.049580E+00 | loss scale: 32768.0 | grad norm: 2.931 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (317.46, 320.05)
    forward-compute ................................: (115.00, 115.11)
    backward-compute ...............................: (202.34, 204.83)
    batch-generator ................................: (0.92, 0.97)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    all-grads-sync .................................: (232.60, 232.68)
    params-all-gather ..............................: (239.95, 240.02)
    optimizer-copy-to-main-grad ....................: (0.99, 1.13)
    optimizer-unscale-and-check-inf ................: (1.23, 1.24)
    optimizer-clip-main-grad .......................: (1.98, 1.99)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (3.78, 3.86)
    optimizer-copy-main-to-model-params ............: (2.86, 3.00)
    optimizer ......................................: (256.86, 256.94)
(min, max) time across ranks (ms):
    evaluate .......................................: (1210.61, 1210.82)
-----------------------------------------------------------------------------------------------
 validation loss at iteration 100 | lm loss value: 7.096642E+00 | lm loss PPL: 1.207904E+03 | 
-----------------------------------------------------------------------------------------------
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 2752698.0 ON nid0703 CANCELLED AT 2024-04-23T03:38:16 ***
++ copy_files
++ copy_files
++ mkdir -p /N/slate/jindjia/bash_scripts/Course/hpc-course/Project/2752698/wandb
++ mkdir -p /N/slate/jindjia/bash_scripts/Course/hpc-course/Project/2752698/wandb
slurmstepd: error: *** JOB 2752698 ON nid0703 CANCELLED AT 2024-04-23T03:38:16 ***
++ mkdir -p /N/slate/jindjia/bash_scripts/Course/hpc-course/Project/2752698/tensorboard
